<!doctype html>
<html lang="en">
<head>
    <title>s3gfault | research archive</title>
    <meta name="description" content="research archive">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link rel="icon" type="image/x-icon" href="/images/favicon.ico">
    <script type="module" src="../js/update-archive.js"></script>
    <link rel="stylesheet" type="text/css" href="../general_page.css">
</head>

<body>
    <div class="crt" id="intro">
        <h1>Research Archive</h1>
        <p><i>Here's a list of research I've worked on.</i></p>
        <!-- <ul id="projects-list"></ul> -->
        <div class="box">
            <b>Enabling Trust in ML Models on Untrusted Edges</b>

            <div class="caption"><i>Python (keras, numpy), differential privacy, long-short term (LSTM) models, split learning</i></div>
            
             Worked alongside PhD mentor in Winter 2022-2023. Proposed a long short-term split (LSTM-SPLIT) machine learning model to help obfuscate private data sent between client-server models on untrustworthy edge nodes.

            <br><br>

            <a href="https://s3gfault.dev/blog/urv">BLOGPOST</a>
        </div>

        <a href="https://s3gfault.dev/" style="text-decoration: none;">Â« Main Page</a>
    </div>
</body>
</html>